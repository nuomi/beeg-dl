#!/usr/bin/env python


import re, urllib2, pickle, sys
import argparse
import os
from download import Download
import pycurl


pageurl_regex = re.compile('http://beeg.com\/\d+')
mp4url_regex = re.compile('http://.+\.mp4')
datafile = 'ret.pk'
ret = []
rate_limit= 40*1024

def extract():

    pageurls = []
    vidzurls = []

    for i in range(2):
        html_src = urllib2.urlopen('http://beeg.com/section/home/'+str(i)+'/').read() 
        matches = pageurl_regex.findall(html_src) 
        pageurls.extend(set(matches))

    for pageurl in pageurls:
        print('looking into '+ pageurl+ '...')
        html_src = urllib2.urlopen(pageurl).read()
        matches = mp4url_regex.findall(html_src)
        vidzurls.extend(set(matches))
        print("\t\tfound "+ str(len(matches)))
    
    pickle.dump(set(vidzurls), open(datafile,'w'))

def curl_progress(total, existing, upload_t, upload_d):
    try:
        frac = float(existing)/float(total)
    except:
        frac = 0
        print "Downloaded %d/%d (%0.2f%%)" % (existing, total, frac)


def download():
    vidzurls = pickle.load(open(datafile))
    for vidzurl in vidzurls:
        filename = os.path.basename(vidzurl)
        c = pycurl.Curl()
        c.setopt(c.URL, vidzurl)
        c.setopt(c.MAX_RECV_SPEED_LARGE, rate_limit)
        if os.path.exists(filename):
            file_id = open(filename, "ab")
            c.setopt(c.RESUME_FROM, os.path.getsize(filename))
        else:
            file_id = open(filename, "wb")

        c.setopt(c.WRITEDATA, file_id)
        c.setopt(c.NOPROGRESS, 0)
        c.setopt(c.PROGRESSFUNCTION, curl_progress)
        c.perform()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='download vidz from beeg.com')
    parser.add_argument('-u',  action='store_true',
                      help='get vidz urls, and save them to a data file.')

    parser.add_argument('-d', action='store_true',
                      help='download vidz according to data file.')

    parser.add_argument('-a', action='store_false',
                      help='get urls and download them.(default)')

    args = vars(parser.parse_args())

    if args['u']:
        extract()
        sys.exit()

    elif args['d']:
        download()
        sys.exit()

    else:
        extract()
        download()
        sys.exit()
